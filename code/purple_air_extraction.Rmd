---
title: "purple_air"
author: "Emma Rieves"
date: "2/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(readr)
library(plyr)
library(sf)
library(raster)
library(rgdal)
library(ggplot2)
library(spacetime)
library(gridExtra)
library(stringr)
library(leaflet)
library(tidygeocoder)
library(parallel)
library(doParallel)

# PM2.5 package recommended by Priyanka
# devtools::install_github("jianzhaobi/bjzresc")
library(bjzresc)

# to make the spherical geometry errors go away
sf::sf_use_s2(FALSE)
```

# Import municipal boundary data
Import the shapefiles that were downloaded from the different city GIS portals. All of this data is stored in the `/GIS_inputs_destruction_fireboundary/` directory.
```{r}
## Use boulder county shapefile to extract municipal boundaries for Boulder counties
BO_CO = st_read("../GIS_inputs_destruction_fireboundary/Boulder_county_munis/Municipalities.shp")

## All of the areas surrounding the fire in shapefiles:
boulder_precincts = st_read("../GIS_inputs_destruction_fireboundary/Unincorporated_Boulder/Unincorporated_Boulder.shp")
# Broomfield, has to be transformed to match the CRS of the Boulder data
broomfield_precincts = st_read("../GIS_inputs_destruction_fireboundary/Broomfield_Precincts/Precincts.shp") %>%
  st_transform(st_crs(boulder_precincts))
# Westminster, has to be transformed to match the CRS of the Boulder data
westminster_city = st_read("../GIS_inputs_destruction_fireboundary/Westminster_CityLimits/CityLimits.shp") %>%
  st_transform(st_crs(boulder_precincts))

# combine all of the Boulder, Broomfield, and Westminster shape data with a union
surrounding_area = st_union(st_combine(boulder_precincts), st_combine(broomfield_precincts)) %>%
  st_union(westminster_city) %>%
  st_combine(.)

# Create an object of all of the places the fire reached from the BO_CO object
# This is needed to get the CRS from
fire_counties = BO_CO %>%
    filter(ZONEDESC == "Louisville" |
             ZONEDESC == "Superior" |
             ZONEDESC == "Broomfield" |
             ZONEDESC == "Lafayette" |
             ZONEDESC == "Boulder")

prg = raster::crs(fire_counties,asText=TRUE)

```

# Download AQ data 
We are using the `bjzresc` package to download the Purple Air sensor data for the region we created above.

This function will get a list of current Purple Air sensors & their locations. We then intersect this list with the municipal boundary geometry we created to get a list of all the sensors within our area of interest. These sensors all have an ID associated with them, which we will then use to download the data for the time period we want.
```{r}
## use bjzresc package to get list of purple air sensors; save to df instead of csv
pa_download = getPurpleairLst(output.path = NULL)
```

```{r}
base_url <- "https://api.purpleair.com/v1/sensors"
sensors_url <- paste(base_url,
                     "?fields=name%2Clocation_type%2Clatitude%2Clongitude",
                     "&",
                     "max_age=0",
                     sep="")

response <- httr::GET(url = sensors_url,
                  add_headers("X-API-Key" = "B991ADB4-748B-11EC-B9BF-42010A800003"),
                  accept_json())

a <- content(response)
b <- do.call(rbind, lapply(a$data, rbind))
b[sapply(b, is.null)] <- NA
sensors <- as.data.frame(b)
colnames(sensors) <- a$fields
sensors <- sensors %>%
  mutate(sensor_index = as.numeric(sensor_index),
         name = as.character(name),
         location_type = as.numeric(location_type),
         Lat = as.numeric(latitude),
         Lon = as.numeric(longitude))

# remove null Lat/Long PA sensors -- important to creating spatial dataframe
sensors <- sensors[!is.na(sensors["Lat"]),]
sensors <- sensors[!is.na(sensors["Lon"]),]

# check that it worked
sum(is.na(sensors[c("Lat","Lon")]))

# create spatial dataframe, set CRS to match muni boundaries
sensors_spatial = sensors %>% 
  st_as_sf(coords = c("Lon", "Lat")) %>% 
  st_set_crs(prg)

raster::crs(sensors_spatial)

# intersect PA download area and fire affected area to download sensors
fire_affected_sensors = st_intersection(sensors_spatial, st_buffer(surrounding_area, 0))

# get sensor IDs for sensors in fire affected area
fire_area_sensor_IDs = fire_affected_sensors$sensor_index

# filter original dataframe to include only sensors in fire affected area
fire_area_sensors = sensors[sensors$sensor_index %in% fire_area_sensor_IDs, ]
```
```{r}
# purpleairDownload(site.csv = fire_area_sensors, start.date = "2021-12-30", end.date = "2022-05-01", output.path = "../fire_counties_PAs/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)

root_url <- "https://june2022.api.purpleair.com/v1/sensors/"
hist_url = '/history'
average_url = '&average=10'
dates_url = '&start_timestamp=2021-12-30&end_timestamp=2022-05-01'
fields_url = paste('&fields=humidity',
                   'temperature',
                   'pressure',
                   # 'pm2.5_a',
                   # 'pm2.5_b',
                   sep="%2C")

foreach(i = 1:nrow(fire_area_sensors)) %dopar% {
 full_url <- paste(root_url,
                   fire_area_sensors$sensor_index[i],
                   hist_url,
                   # average_url,
                   dates_url,
                   fields_url,
                   sep="")
 response <- httr::GET(url = full_url,
                       add_headers("X-API-Key" = "B991ADB4-748B-11EC-B9BF-42010A800003"))
 print(response)
}
```


Here we are intersecting the sensor list with our geometry to find sensors within the fire area.
```{r}
## Intersect municipal boundaries with PA sensors

# remove null Lat/Long PA sensors -- important to creating spatial dataframe
pa_download = pa_download[complete.cases(pa_download[c("Lat","Lon")]),]

# check that it worked
sum(is.na(pa_download[c("Lat", "Lon")]))

# create spatial dataframe, set CRS to match muni boundaries
pa_download_spatial = pa_download %>% 
  st_as_sf(coords = c("Lon", "Lat")) %>% 
  st_set_crs(prg)

# check CRS
raster::crs(pa_download_spatial)

# intersect PA download area and fire affected area to download sensors
fire_affected_sensors = st_intersection(pa_download_spatial, st_buffer(surrounding_area, 0))

# get sensor IDs for sensors in fire affected area
fire_area_sensor_IDs = fire_affected_sensors$ID

# filter original dataframe to include only sensors in fire affected area
(fire_area_sensors = pa_download[pa_download$ID %in% fire_area_sensor_IDs, ])
```

For the next chunk of code to run, if on a Windows machine you **must** do the following:
1. In the console, run `trace("purpleairDownload", edit=TRUE)`
2. A window will pop up with the source code for the download function
```{r eval=F}
# download purple air data -- TAKES A LONG TIME TO RUN SO BE READY FOR THAT
## output path is a folder that stores a csv for each sensor for the target time period
## average means that data is averaged for 10-minute intervals
## indoor = TRUE includes indoor sensor observations

# for windows machines only
# the following line will create a local 4-node snow cluster
workers = makeCluster(4, type="SOCK", outfile="Log.txt")
registerDoParallel(workers)

purpleairDownload(site.csv = fire_area_sensors, start.date = "2021-12-30", end.date = "2022-05-01", output.path = "../fire_counties_PAs/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)
```

```{r}
# download purple air data (with above specifications) for the Marshall fire boundary
#purpleairDownload(site.csv = fire_area_sensors2, start.date = "2021-12-30", end.date = "2022-05-01", output.path = "marshall_fire_path_PAs/", average = 10, time.zone = "America/Denver", indoor = TRUE, n.thread = 1)
```


# Process and clean data

## Helper function -- OLD
```{r}
# OLD HELPER FUNCTION USED TO DETERMINE LOCATION
# get_sensor_info <- function(id) {
#   # for each id, get the rest of the information (including lat/long)
#   info = id_key[id_key$ID == id,]
#   # create a spatial version of info
#   info_spatial = st_as_sf(info, coords = c("Lon","Lat"), crs = prg)
#   #intersect ID info with the county map to determine location
#   city_intersect = st_intersection(info_spatial,st_buffer(fire_counties,0))["ZONEDESC"]
#   city = city_intersect$ZONEDESC
#   # return dataframe with: ID, lat/long, indoor/outdoor, and city
#   info = info %>%
#     mutate(city = ifelse(length(city) == 0, NA, city)) %>%
#     dplyr::select(-Name)
#   return(city)
# }

# id_key %>% rowwise() %>% mutate(place = get_sensor_info(ID))
# 
# 
# # use helper function to get municipality for each sensor
# sensor_info = get_sensor_info(id_key$ID)
# 
# sensor_info %>% group_by(city) %>% dplyr::summarise(n = n())
```


## Combine & clean data
```{r}
## Read downloaded PA files from their filepath and turn them into a DF

# directory where files are stored
dir = "../fire_counties_PAs/"

# create a list of all file names in this directory
file_name = list.files(path=dir, pattern="*.csv", full.names=TRUE)

# read csvs for each filename in list --> results in a list of lists
AQ_files = lapply(file_name, read_csv)

# combine all AQ lists from each directory into AQ dataframe 
(AQ_df = rbind.fill(AQ_files) %>% as.data.frame())

# create a key coordinating the sensor ID number to its lat/lon
(id_key = AQ_df %>%
  group_by(ID) %>%
  dplyr::select(ID, Lon, Lat, Name, Location) %>%
  unique() %>%
  # remove NAs for spatial intersections to occur
  na.omit()
  )
```

```{r}
# create a date column in POSIX format to create time series
AQ_df$datetime = as.POSIXct(AQ_df$created_at)

# rename and select important columns 
AQ_df = AQ_df %>% 
  dplyr::rename(pm25_a = `PM2.5_CF_ATM_ug/m3_A`,
                 pm25_b = `PM2.5_CF_ATM_ug/m3_B`,
                 temp = Temperature_F_A,
                 rh = `Humidity_%_A`)

# create hourly pm column
AQ_df$hour = as.POSIXlt(AQ_df$datetime)$hour

# Convert numeric values to a numeric class
AQ_df$pm25_a = as.numeric(AQ_df$pm25_a)
AQ_df$pm25_b = as.numeric(AQ_df$pm25_b)
AQ_df$temp = as.numeric(AQ_df$temp)
AQ_df$rh = as.numeric(AQ_df$rh)
```


```{r}
# fill in time series
(AQ_df = AQ_df %>%
  dplyr::select(ID, Name, Lon, Lat, Location, datetime, pm25_a, pm25_b, temp, rh) %>%
  group_by(ID) %>%
  # add in NAs to timeseries to calculate % complete (before missing time periods were just absent and not NAs in the dataset)
  complete(datetime = seq(min(datetime), max(datetime), by = "10 min")))

```
## geocode
```{r}
# GEOCODE
# takes a while to run.. uses lat/long to "reverse" geocode (with tidygeocoder package) and provide address (including city/zip)
sensor_info = reverse_geocode(id_key,lat=Lat,long = Lon)

# clean address to extract zip code and city name (city update)
# reviewing this shows that some cities didn't turn out correctly.. I just looked up addresses
##### Rock Creek Ranch II -- Superior
##### Broadway -- Boulder (address on Broadway)
sensor_info = sensor_info %>% mutate(address_split = sub(", United States.*","",address),
                        zip_code = str_extract(address_split, "\\w+$"),
                        address_split2 = sub(", Boulder County.*","",address),
                        city = str_extract(address_split2, "\\w+$"),
                        # recode cities that didn't show up properly
                        city_update = ifelse(city == "II", "Superior",
                                             ifelse(city == "Broadway", "Boulder",city))) %>%
  dplyr::select(-c(address_split,address_split2,city))

# check distribution of sensors
sensor_info %>% group_by(city_update) %>% dplyr::summarise(n = n())
```
## add in classifications for "fire classification" and data completeness by fire period and month
```{r}
## NEED TO RECLASSIFY FIRE PERIODS

(time_period_classification = AQ_df %>%
  mutate(time_period = ifelse(datetime<as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")), "pre_fire_period",
                              ifelse(datetime>=as.POSIXct(strptime("2021-12-30 10:00:00", "%Y-%m-%d %H:%M:%S")) & datetime<= as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "fire_period",
                                     ifelse(datetime>as.POSIXct(strptime("2022-01-01 11:59:59", "%Y-%m-%d %H:%M:%S")), "post_fire_period", "other time")))) %>%
  group_by(ID,time_period) %>%
  dplyr::summarize(
    complete_a= sum(complete.cases(pm25_a))/n()*100
    #complete_b = sum(complete.cases(pm25_b))/n()*100
  ) %>%
  pivot_wider(names_from = time_period, values_from = complete_a) %>%
  rowwise() %>%
  mutate(Status = case_when(
    fire_period >= 75 & post_fire_period >= 85 & pre_fire_period >= 95 ~ "Complete data throughout fire period",
    fire_period < 75 & post_fire_period >= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, returned online",
    fire_period < 75 & post_fire_period <= 75 & pre_fire_period >= 95 ~ "Sensor offline during fire, did not return online",
    fire_period < 75 & post_fire_period <= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, did not return online",
    fire_period < 75 & post_fire_period >= 75 & is.na(pre_fire_period) ~ "Sensor added during fire, returned online",
    is.na(fire_period) & is.na(pre_fire_period) ~ "Sensor came online after fire"
  )))

(month_added = AQ_df %>%
  group_by(ID) %>%
  dplyr::select(ID, datetime) %>%
  dplyr::summarize(Month = ifelse(format(datetime[1], "%m-%Y") == "12-2021","Before or during 12-2021",format(datetime[1], "%m-%Y"))))

# merge time period classification with month added, then merge with the sensor info (contains address, zip, city)
sensor_data = merge(time_period_classification,month_added,by="ID")
(sensor_data_full = merge(sensor_data,sensor_info,by="ID"))


## factor to order time periods and status
sensor_data_full$Month = factor(sensor_data_full$Month, levels = c("Before or during 12-2021","01-2022","02-2022","03-2022","04-2022"))
sensor_data_full$Status = factor(sensor_data_full$Status, levels = c("Complete data throughout fire period", "Sensor offline during fire, returned online", "Sensor offline during fire, did not return online", "Sensor added during fire, returned online", "Sensor added during fire, did not return online", "Sensor came online after fire"))

```


# Export data
## Data for AQ cleaning and analysis
```{r}
# AQ df for correction script
write.csv(AQ_df,"../intermediary_outputs/aq_data.csv")
```

## Sensor information for mapping
```{r}
# sensor info for visualization script
write.csv(sensor_data_full,"../intermediary_outputs/sensor_data_full.csv")
```


